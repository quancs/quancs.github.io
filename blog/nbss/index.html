<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<title>Narrow-band Speech Separation - Changsheng Quan</title>
<meta name=description content="Narrow-band Speech Separation with Full-band Permutation Invariant Training">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=twitter:card content="summary_large_image">
<meta property="og:site_name" content="Changsheng Quan">
<meta property="og:title" content="Narrow-band Speech Separation">
<meta property="og:description" content="Narrow-band Speech Separation with Full-band Permutation Invariant Training">
<meta property="og:type" content="article">
<meta property="og:url" content="https://quancs.github.io/blog/nbss/">
<meta property="og:image" content="https://quancs.github.io/img/main/logo.jpg">
<link rel="shortcut icon" href="/favicon.ico?v=1">
<meta name=generator content="Hugo 0.89.4">
<link rel=stylesheet href=/css/bundle.min.95cbbf94d60e35b3dbbb8f4b3e4cb5047cf01280e691386b07a7f74d67068512.css integrity="sha256-lcu/lNYONbPbu49LPky1BHzwEoDmkThrB6f3TWcGhRI="><link rel=stylesheet href=/css/add-on.css>
</head>
<body>
<header id=site-header>
<nav id=site-nav>
<h1 class=nav-title>
<a href=/ class=nav>
Blog
</a>
</h1>
<menu id=site-nav-menu class="flyout-menu menu">
<a href=/ class="nav link"><i class="fa fa-home"></i> Home</a>
<a href=/about class="nav link"><i class="far fa-id-card"></i> About Me</a>
<a href=/blog class="nav link"><i class="far fa-newspaper"></i> Blog</a>
<a href=/contact class="nav link"><i class="far fa-envelope"></i> Contact</a>
<a href=/publication class="nav link"><i class="far fa-newspaper"></i> Publication</a>
<a href=#share-menu class="nav link share-toggle"><i class="fas fa-share-alt">&nbsp;</i>Share</a>
<a href=#search-input class="nav link search-toggle"><i class="fas fa-search">&nbsp;</i>Search</a>
</menu>
<a href=#search-input class="nav search-toggle"><i class="fas fa-search fa-2x">&nbsp;</i></a>
<a href=#share-menu class="nav share-toggle"><i class="fas fa-share-alt fa-2x">&nbsp;</i></a>
<a href=#lang-menu class="nav lang-toggle" lang=en>en</a>
<a href=#site-nav class="nav nav-toggle"><i class="fas fa-bars fa-2x"></i></a>
</nav>
<menu id=search class=menu><input id=search-input class="search-input menu"></input><div id=search-results class="search-results menu"></div></menu>
<menu id=lang-menu class="flyout-menu menu">
<a href=# lang=en class="nav link active">English (en)</a>
<a href=https://quancs.github.io/zh-cn/blog/nbss/ lang=zh-cn class="nav link">中文 (简体) (zh-cn)</a>
</menu>
<menu id=share-menu class="flyout-menu menu">
<h1>Share Post</h1>
</menu>
</header>
<div id=wrapper>
<section id=site-intro class=hidden-single-column>
<a href=/><img src=/img/main/logo.jpg class=circle width=100 alt="Changsheng Quan's Photo"></a>
<header>
<h1>Changsheng Quan</h1>
</header>
<main>
<p>A PhD Student of Computer Science</p>
</main>
<footer>
<ul class=socnet-icons>
<li><a href=//github.com/quancs target=_blank rel=noopener title=GitHub class="fab fa-github"></a></li>
<li><a href="//wpa.qq.com/msgrd?v=3&uin=1017241746&site=qq&menu=yes" target=_blank rel=noopener title=QQ class="fab fa-qq"></a></li>
<li><a href="//scholar.google.com/citations?user=prTK3NwAAAAJ" target=_blank rel=noopener title="Google Scholar"><i class="ai ai-google-scholar"></i></a></li>
<li><a href=mailto:quancs@qq.com target=_blank title=Email class="far fa-envelope"></a></li>
</ul>
</footer>
</section>
<main id=site-main>
<article>
<div class=post>
<header>
<div class=title>
<h2><a href=/blog/nbss/>Narrow-band Speech Separation</a></h2>
<p>Narrow-band Speech Separation with Full-band Permutation Invariant Training</p>
</div>
<div class=meta>
<time datetime="2021-12-04 00:00:00 +0000 UTC">December 4, 2021</time>
<p>quancs</p>
<p>3-Minute Read</p>
</div>
</header>
<div id=socnet-share>
</div>
<div class=content>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<h2 id=abstract>Abstract</h2>
<p>This paper addresses the problem of multi-channel multi-speech separation based on deep learning techniques.
In the short time Fourier transform domain, we propose an end-to-end narrow-band network that directly takes as input the multi-channel mixture signals of one frequency, and outputs the separated signals of this frequency.
In narrow-band, the spatial information (or inter-channel difference) can well discriminate between speakers at different positions.
This information is intensively used in many narrow-band speech separation methods, such as beamforming and clustering of spatial vectors.
The proposed network is trained to learn a rule to automatically exploit this information and perform speech separation.
Such a rule should be valid for any frequency, thence the network is shared by all frequencies.
In addition, a full-band permutation invariant training criterion is proposed to solve the frequency permutation problem encountered by most narrow-band methods.
Experiments show that, by focusing on deeply learning the narrow-band information, the proposed method outperforms the oracle beamforming method and the state-of-the-art deep learning based method.</p>
<h2 id=method>Method</h2>
<p>This method includes two main parts:
the first part is to separate in each frequency;
the second part is about how to solve the frequency permutation problem to train the network.</p>
<p>The following content assumes we have <em>M</em> microphones and <em>N</em> speakers.</p>
<h3 id=1-narrow-band-deep-speech-separation>1) Narrow-band Deep Speech Separation</h3>
<p><strong>Prepare Narrow-band Input.</strong>
Why Narrow-band? It carries spatial information of speakers</p>
<p><img src=/blog/NBSS_examples/narrow_band_input.jpg alt=image title="from time domain to narrow-band input"></p>
<p><strong>Network Processing:</strong> Train network to separate different speakers in narrow-band</p>
<p><img src=/blog/NBSS_examples/network_processing.jpg alt=image2 title="Send narrow-band input to the network"></p>
<h3 id=2-full-band-permutation-invariant-training>2) Full-band Permutation Invariant Training</h3>
<p><strong>Frequency Permutation Problem</strong>&mdash;How to find the correspondence of separated signals accross frequencies? One possible frequency permutation for two-speaker case:</p>
<p><img src=/blog/NBSS_examples/fpp.jpg alt=image3></p>
<p>Our solution for FPP: <strong>Frequency Binding</strong>&mdash;Force the separated signals at the same output position to belong to the same speaker</p>
<p><img src=/blog/NBSS_examples/frequency_binding.jpg alt=image4></p>
<p><strong>Loss Calculation</strong></p>
<p>$$ fPIT(\boldsymbol{\rm \widehat{Y}}^{1},\ldots, \boldsymbol{\rm \widehat{Y}}^{N}, \boldsymbol{\rm {Y}}^{1},\ldots,\boldsymbol{\rm {Y}}^{N})=\mathop{min}_{p\in \mathcal{P}}\frac{1}{N}\sum_n \mathcal{L}(\boldsymbol{{\rm {Y}}}^n,\boldsymbol{{\rm \widehat{Y}}}^{p(n)}) $$</p>
<p>where <em>P</em> is the set of all possible frequency permutations, and <em>p</em> is one possible frequency permutation in <em>P</em>.
And the negative SI-SDR [1] is used as the loss function for each prediction-target pair.</p>
<h2 id=results>Results</h2>
<p>Performance Comparision with SOTA Speech Separation Methods for 8-Channel 2-Speaker Mixtures</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>SDR</th>
<th>SI-SDR</th>
<th>NB-PESQ</th>
<th>WB-PESQ</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mixture</td>
<td>0.18</td>
<td>0.00</td>
<td>2.05</td>
<td>1.6</td>
</tr>
<tr>
<td>Oracle MVDR [2]</td>
<td>8.15</td>
<td>4.58</td>
<td>3.20</td>
<td>2.62</td>
</tr>
<tr>
<td>FaSNet-TAC [3]</td>
<td>12.81</td>
<td>12.26</td>
<td>2.92</td>
<td>2.49</td>
</tr>
<tr>
<td>prop.</td>
<td><strong>13.89</strong></td>
<td><strong>13.26</strong></td>
<td><strong>3.31</strong></td>
<td><strong>2.87</strong></td>
</tr>
</tbody>
</table>
<h2 id=examples>Examples</h2>
<table>
<thead>
<tr>
<th>Examples</th>
<th>Mix</th>
<th>Oracle MVDR</th>
<th>FaSNet-TAC</th>
<th>prop.</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td><audio controls src=/blog/NBSS_examples/1_mix.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/1_spk1_p_MVDR.wav></audio> </br> <audio controls src=/blog/NBSS_examples/1_spk2_p_MVDR.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/1_spk1_p_TAC.wav></audio> </br> <audio controls src=/blog/NBSS_examples/1_spk2_p_TAC.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/1_spk1_p_NBSS.wav></audio> </br> <audio controls src=/blog/NBSS_examples/1_spk2_p_NBSS.wav></audio></td>
</tr>
<tr>
<td>2</td>
<td><audio controls src=/blog/NBSS_examples/0_mix.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/0_spk1_p_MVDR.wav></audio> </br> <audio controls src=/blog/NBSS_examples/0_spk2_p_MVDR.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/0_spk1_p_TAC.wav></audio> </br> <audio controls src=/blog/NBSS_examples/0_spk2_p_TAC.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/0_spk1_p_NBSS.wav></audio> </br> <audio controls src=/blog/NBSS_examples/0_spk2_p_NBSS.wav></audio></td>
</tr>
<tr>
<td>3</td>
<td><audio controls src=/blog/NBSS_examples/2_mix.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/2_spk1_p_MVDR.wav></audio> </br> <audio controls src=/blog/NBSS_examples/2_spk2_p_MVDR.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/2_spk1_p_TAC.wav></audio> </br> <audio controls src=/blog/NBSS_examples/2_spk2_p_TAC.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/2_spk1_p_NBSS.wav></audio> </br> <audio controls src=/blog/NBSS_examples/2_spk2_p_NBSS.wav></audio></td>
</tr>
<tr>
<td>4</td>
<td><audio controls src=/blog/NBSS_examples/3_mix.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/3_spk1_p_MVDR.wav></audio> </br> <audio controls src=/blog/NBSS_examples/3_spk2_p_MVDR.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/3_spk1_p_TAC.wav></audio> </br> <audio controls src=/blog/NBSS_examples/3_spk2_p_TAC.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/3_spk1_p_NBSS.wav></audio> </br> <audio controls src=/blog/NBSS_examples/3_spk2_p_NBSS.wav></audio></td>
</tr>
<tr>
<td>5</td>
<td><audio controls src=/blog/NBSS_examples/4_mix.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/4_spk1_p_MVDR.wav></audio> </br> <audio controls src=/blog/NBSS_examples/4_spk2_p_MVDR.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/4_spk1_p_TAC.wav></audio> </br> <audio controls src=/blog/NBSS_examples/4_spk2_p_TAC.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/4_spk1_p_NBSS.wav></audio> </br> <audio controls src=/blog/NBSS_examples/4_spk2_p_NBSS.wav></audio></td>
</tr>
<tr>
<td>6</td>
<td><audio controls src=/blog/NBSS_examples/5_mix.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/5_spk1_p_MVDR.wav></audio> </br> <audio controls src=/blog/NBSS_examples/5_spk2_p_MVDR.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/5_spk1_p_TAC.wav></audio> </br> <audio controls src=/blog/NBSS_examples/5_spk2_p_TAC.wav></audio></td>
<td><audio controls src=/blog/NBSS_examples/5_spk1_p_NBSS.wav></audio> </br> <audio controls src=/blog/NBSS_examples/5_spk2_p_NBSS.wav></audio></td>
</tr>
</tbody>
</table>
<h2 id=source-code>Source Code</h2>
<p>This work is open sourced at github, see <strong><a href=https://github.com/quancs/NBSS>[<font color=DarkOrchid>code</font>]</a></strong> and <strong><a href=https://arxiv.org/pdf/2110.05966>[<font color=DarkOrchid>pdf</font>]</a></strong>. If you like this work and are willing to cite us, please use:</p>
<pre tabindex=0><code>@article{quan_multi-channel_2021,
	title = {Multi-channel {Narrow}-{Band} {Deep} {Speech} {Separation} with {Full}-band {Permutation} {Invariant} {Training}},
	journal = {arXiv preprint arXiv:2110.05966},
	author = {Quan, Changsheng and Li, Xiaofei},
	year = {2021},
}
</code></pre><h2 id=references>References</h2>
<p>[1] Robin Scheibler. 2021. SDR &ndash; Medium Rare with Fast Computations. arXiv:2110.06440.</p>
<p>[2] <a href=https://github.com/Enny1991/beamformers>https://github.com/Enny1991/beamformers</a></p>
<p>[3] Yi Luo, Zhuo Chen, Nima Mesgarani, and Takuya Yoshioka. End-to-end Microphone Permutation and Number Invariant Multi-channel Speech Separation. In ICASSP 2020.</p>
</div>
<footer>
<div class=stats>
</div>
</footer>
</div>
</article>
<div class=pagination>
</div>
</main>
<section id=site-sidebar>
<section id=recent-posts>
<header>
<h1>Recent Posts</h1>
</header>
<article class=mini-post>
<header>
<h2><a href=/blog/nbss/>Narrow-band Speech Separation</a></h2>
<time class=published datetime="2021-12-04 00:00:00 +0000 UTC">December 4, 2021</time>
</header>
</article>
</section>
</section>
<footer id=site-footer>
<p class=copyright>
© 2021 Changsheng Quan
<br>
Theme: <a href=https://github.com/pacollins/hugo-future-imperfect-slim target=_blank rel=noopener>Hugo Future Imperfect Slim</a><br>A <a href=https://html5up.net/future-imperfect target=_blank rel=noopener>HTML5 UP port</a> | Powered by <a href=https://gohugo.io/ title=0.89.4 target=_blank rel=noopener>Hugo</a>
</p>
</footer>
<a id=back-to-top href=# class="fas fa-arrow-up fa-2x"></a>
<script src=/js/highlight.js></script>
<script>hljs.highlightAll()</script><script src=/js/bundle.min.fe20538ce3dbce47ceabaf2b70a5988bbf632d80b81634a2e5c93a4834e2cd9e.js integrity="sha256-/iBTjOPbzkfOq68rcKWYi79jLYC4FjSi5ck6SDTizZ4="></script>
<script src=/js/add-on.js></script>
</div>
</body>
</html>