[{"title":"Narrow-band Deep Speech Separation","date":"","description":"Multi-channel Narrow-band Deep Speech Separation with Full-band Permutation Invariant Training","body":"  Abstract This paper addresses the problem of multi-channel multi-speech separation based on deep learning techniques. In the short time Fourier transform domain, we propose an end-to-end narrow-band network that directly takes as input the multi-channel mixture signals of one frequency, and outputs the separated signals of this frequency. In narrow-band, the spatial information (or inter-channel difference) can well discriminate between speakers at different positions. This information is intensively used in many narrow-band speech separation methods, such as beamforming and clustering of spatial vectors. The proposed network is trained to learn a rule to automatically exploit this information and perform speech separation. Such a rule should be valid for any frequency, thence the network is shared by all frequencies. In addition, a full-band permutation invariant training criterion is proposed to solve the frequency permutation problem encountered by most narrow-band methods. Experiments show that, by focusing on deeply learning the narrow-band information, the proposed method outperforms the oracle beamforming method and the state-of-the-art deep learning based method.\nMethod This method includes two main parts: the first part is to separate in each frequency; the second part is about how to solve the frequency permutation problem to train the network.\nThe following content assumes we have M microphones and N speakers.\n1) Narrow-band Deep Speech Separation Prepare Narrow-band Input. Why Narrow-band? It carries spatial information of speakers\nNetwork Processing: Train network to separate different speakers in narrow-band\n2) Full-band Permutation Invariant Training Frequency Permutation Problem\u0026mdash;How to find the correspondence of separated signals accross frequencies? One possible frequency permutation for two-speaker case:\nOur solution for FPP: Frequency Binding\u0026mdash;Force the separated signals at the same output position to belong to the same speaker\nLoss Calculation\n$$ fPIT(\\boldsymbol{\\rm \\widehat{Y}}^{1},\\ldots, \\boldsymbol{\\rm \\widehat{Y}}^{N}, \\boldsymbol{\\rm {Y}}^{1},\\ldots,\\boldsymbol{\\rm {Y}}^{N})=\\mathop{min}_{p\\in \\mathcal{P}}\\frac{1}{N}\\sum_n \\mathcal{L}(\\boldsymbol{{\\rm {Y}}}^n,\\boldsymbol{{\\rm \\widehat{Y}}}^{p(n)}) $$\nwhere P is the set of all possible frequency permutations, and p is one possible frequency permutation in P. And the negative SI-SDR [1] is used as the loss function for each prediction-target pair.\n3) Narrow-band Conformer The narrow-band conformer (NBC) proposed in [6] is used to replace the BiLSTM network used in [5], as the narrow-band speech separation shares a similar principle with the self-attention mechanism and convolutions in Conformer. The narrow-band conformer structure:\n Results Performance Comparision with SOTA Speech Separation Methods for 8-Channel 2-Speaker Mixtures (reported in narrow-band conformer [6])\n   Model #param NB-PESQ WB-PESQ SI-SDR RTF     Mixture - 2.05 1.59 0.0 -   Oracle MVDR [2] - 3.16 2.65 11.0 -   FaSNet-TAC [3] 2.8 M 2.96 2.53 12.6 0.67   SepFormer [4] 25.7 M 3.17 2.72 13.2 1.69   SepFormerMC 25.7 M 3.42 3.01 14.9 1.70   NB-BLSTM [5] 1.2 M 3.28 2.81 12.8 0.37   NBC [6] 2.0 M 4.00 3.78 20.3 1.32    Examples New examples of Narrow-band Conformer is coming soon.\n   Examples Mix Oracle MVDR [2] FaSNet-TAC [3] prop.     1             2             3             4             5             6              Source Code This work is open sourced at github, see [code], [NBSS pdf], [Narrow-band Conformer pdf]. If you like this work and are willing to cite us, please use:\n@inproceedings{quan_multi-channel_2022, title = {Multi-channel {Narrow}-band {Deep} {Speech} {Separation} with {Full}-band {Permutation} {Invariant} {Training}}, booktitle = {{ICASSP}}, author = {Quan, Changsheng and Li, Xiaofei}, year = {2022}, } and\n@article{quan_multichannel_2022, title = {Multichannel {Speech} {Separation} with {Narrow}-band Conformer}, journal = {arXiv preprint arXiv:2204.04464}, author = {Quan, Changsheng and Li, Xiaofei}, year = {2022}, } References [1] Jonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R. Hershey. SDR – Half-baked or Well Done? In ICASSP 2019.\n[2] https://github.com/Enny1991/beamformers\n[3] Yi Luo, Zhuo Chen, Nima Mesgarani, and Takuya Yoshioka. End-to-end Microphone Permutation and Number Invariant Multi-channel Speech Separation. In ICASSP 2020.\n[4] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong. Attention Is All You Need In Speech Separation. In ICASSP 2021.\n[5] Changsheng Quan, Xiaofei Li. Multi-channel Narrow-band Deep Speech Separation with Full-band Permutation Invariant Training. In ICASSP 2022.\n[6] Changsheng Quan, Xiaofei Li. Multichannel Speech Separation with Narrow-band Conformer. arXiv preprint arXiv:2204.04464.\n","ref":"/blog/nbss/"},{"title":"Publication","date":"","description":"When the going gets tough, the tough get going.","body":"2021 [4]\tChangsheng Quan and Xiaofei Li. 2021. Multi-channel Narrow-Band Deep Speech Separation with Full-band Permutation Invariant Training. arXiv preprint arXiv:2110.05966 (2021). [examples and introduction] [code] [pdf]\n[3]\tChangsheng Quan and Ping Guo. 2021. A local search method based on edge age strategy for minimum vertex cover problem in massive graphs. Expert Systems with Applications 182, (November 2021), 115185. DOI:https://doi.org/10.1016/j.eswa.2021.115185 [code]\n2019 [2]\tPing Guo, Changsheng Quan, and Haizhu Chen. 2019. MEAMVC: A Membrane Evolutionary Algorithm for Solving Minimum Vertex Cover Problem. IEEE Access 7, (2019), 60774–60784. DOI:https://doi.org/10.1109/ACCESS.2019.2915550 [code]\n[1]\tPing Guo, Changsheng Quan, and Lian Ye. 2019. UPSimulator: A general P system simulator. Knowledge-Based Systems 170, (April 2019), 20–25. DOI:https://doi.org/10.1016/j.knosys.2019.01.013 [code]\n","ref":"/publication/"},{"title":"About Me","date":"","description":"A Computer Science PhD Student","body":"Personal Profile I am a joint Ph.D. student of Westlake University and Zhejiang University, advised by Dr. Xiaofei Li. Also, I am a member of Audio Laboratory in Westlake University, where I do my research work on Machine Learning, and Speech Separation.\nResearch My research interests include Machine Learning and Audio Signal Processing. Especially in, Speech Separation, Enhancement, ASR.\nMore Github, Google Scholar\n","ref":"/about/"},{"title":"Contact","date":"","description":"","body":"","ref":"/contact/"}]