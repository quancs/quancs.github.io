[{"title":"Narrow-band Speech Separation","date":"","description":"Narrow-band Speech Separation with Full-band Permutation Invariant Training","body":"  Abstract This paper addresses the problem of multi-channel multi-speech separation based on deep learning techniques. In the short time Fourier transform domain, we propose an end-to-end narrow-band network that directly takes as input the multi-channel mixture signals of one frequency, and outputs the separated signals of this frequency. In narrow-band, the spatial information (or inter-channel difference) can well discriminate between speakers at different positions. This information is intensively used in many narrow-band speech separation methods, such as beamforming and clustering of spatial vectors. The proposed network is trained to learn a rule to automatically exploit this information and perform speech separation. Such a rule should be valid for any frequency, thence the network is shared by all frequencies. In addition, a full-band permutation invariant training criterion is proposed to solve the frequency permutation problem encountered by most narrow-band methods. Experiments show that, by focusing on deeply learning the narrow-band information, the proposed method outperforms the oracle beamforming method and the state-of-the-art deep learning based method.\nMethod This method includes two main parts: the first part is to separate in each frequency; the second part is about how to solve the frequency permutation problem to train the network.\nThe following content assumes we have M microphones and N speakers.\n1) Narrow-band Deep Speech Separation Prepare Narrow-band Input. Why Narrow-band? It carries spatial information of speakers\nNetwork Processing: Train network to separate different speakers in narrow-band\n2) Full-band Permutation Invariant Training Frequency Permutation Problem\u0026mdash;How to find the correspondence of separated signals accross frequencies? One possible frequency permutation for two-speaker case:\nOur solution for FPP: Frequency Binding\u0026mdash;Force the separated signals at the same output position to belong to the same speaker\nLoss Calculation\n$$ fPIT(\\boldsymbol{\\rm \\widehat{Y}}^{1},\\ldots, \\boldsymbol{\\rm \\widehat{Y}}^{N}, \\boldsymbol{\\rm {Y}}^{1},\\ldots,\\boldsymbol{\\rm {Y}}^{N})=\\mathop{min}_{p\\in \\mathcal{P}}\\frac{1}{N}\\sum_n \\mathcal{L}(\\boldsymbol{{\\rm {Y}}}^n,\\boldsymbol{{\\rm \\widehat{Y}}}^{p(n)}) $$\nwhere P is the set of all possible frequency permutations, and p is one possible frequency permutation in P. And the negative SI-SDR [1] is used as the loss function for each prediction-target pair.\nResults Performance Comparision with SOTA Speech Separation Methods for 8-Channel 2-Speaker Mixtures\n   Model SDR SI-SDR NB-PESQ WB-PESQ     Mixture 0.18 0.00 2.05 1.6   Oracle MVDR [2] 8.15 4.58 3.20 2.62   FaSNet-TAC [3] 12.81 12.26 2.92 2.49   prop. 13.89 13.26 3.31 2.87    Examples    Examples Mix Oracle MVDR FaSNet-TAC prop.     1             2             3             4             5             6              Source Code This work is open sourced at github, see [code] and [pdf]. If you like this work and want to cite us, please use:\n@article{quan_multi-channel_2021, title = {Multi-channel {Narrow}-{Band} {Deep} {Speech} {Separation} with {Full}-band {Permutation} {Invariant} {Training}}, journal = {arXiv preprint arXiv:2110.05966}, author = {Quan, Changsheng and Li, Xiaofei}, year = {2021}, } References [1] Robin Scheibler. 2021. SDR \u0026ndash; Medium Rare with Fast Computations. arXiv:2110.06440.\n[2] https://github.com/Enny1991/beamformers\n[3] Yi Luo, Zhuo Chen, Nima Mesgarani, and Takuya Yoshioka. End-to-end Microphone Permutation and Number Invariant Multi-channel Speech Separation. In ICASSP 2020.\n","ref":"/blog/nbss/"},{"title":"Publication","date":"","description":"When the going gets tough, the tough get going.","body":"2021 [4]\tChangsheng Quan and Xiaofei Li. 2021. Multi-channel Narrow-Band Deep Speech Separation with Full-band Permutation Invariant Training. arXiv preprint arXiv:2110.05966 (2021). [examples and introduction] [code] [pdf]\n[3]\tChangsheng Quan and Ping Guo. 2021. A local search method based on edge age strategy for minimum vertex cover problem in massive graphs. Expert Systems with Applications 182, (November 2021), 115185. DOI:https://doi.org/10.1016/j.eswa.2021.115185 [code]\n2019 [2]\tPing Guo, Changsheng Quan, and Haizhu Chen. 2019. MEAMVC: A Membrane Evolutionary Algorithm for Solving Minimum Vertex Cover Problem. IEEE Access 7, (2019), 60774–60784. DOI:https://doi.org/10.1109/ACCESS.2019.2915550 [code]\n[1]\tPing Guo, Changsheng Quan, and Lian Ye. 2019. UPSimulator: A general P system simulator. Knowledge-Based Systems 170, (April 2019), 20–25. DOI:https://doi.org/10.1016/j.knosys.2019.01.013 [code]\n","ref":"/publication/"},{"title":"About Me","date":"","description":"A Computer Science PhD Student","body":"Personal Profile I am a joint Ph.D. student of Westlake University and Zhejiang University, advised by Dr. Xiaofei Li. Also, I am a member of Audio Laboratory in Westlake University, where I do my research work on Machine Learning, and Speech Separation.\nResearch My research interests include Machine Learning and Audio Signal Processing. Especially in, Speech Separation, Enhancement, ASR.\nMore Github, Google Scholar\n","ref":"/about/"},{"title":"Contact","date":"","description":"","body":"","ref":"/contact/"}]